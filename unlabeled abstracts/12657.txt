Learning about a Moving Target in Resource Management: Optimal Bayesian Disease Control
Resource managers must often make difficult choices in the face of imperfectly observed and dynamically changing systems (e.g., livestock, fisheries, water, and invasive species). A rich set of techniques exists for identifying optimal choices when that uncertainty is assumed to be understood and irreducible. Standard optimization approaches, however, cannot address situations in which reducible uncertainty applies to either system behavior or environmental states. The adaptive management literature overcomes this limitation with tools for optimal learning, but has been limited to highly simplified models with state and action spaces that are discrete and small. We overcome this problem by using a recently developed extension of the Partially Observable Markov Decision Process (POMDP) framework to allow for learning about a continuous state. We illustrate this methodology by exploring optimal control of bovine tuberculosis in New Zealand cattle. Disease testing--the control variable--serves to identify herds for treatment and provides information on prevalence, which is both imperfectly observed and subject to change due to controllable and uncontrollable factors. We find substantial efficiency losses from both ignoring learning (standard stochastic optimization) and from simplifying system dynamics (to facilitate a typical, simple learning model), though the latter effect dominates in our setting. We also find that under an adaptive management approach, simplifying dynamics can lead to a belief trap in which information gathering ceases, beliefs become increasingly inaccurate, and losses abound.
10.1093/AJAE/AAW033