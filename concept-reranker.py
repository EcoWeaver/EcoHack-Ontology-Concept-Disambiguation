# -*- coding: utf-8 -*-
"""reranker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LVp0Zz9eB_W4osc4ZpejvyDOmNTtMg0S
"""

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
import numpy as np
import re

# Define the device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Example data loading functions
def load_abstracts():
    # Example abstracts dataset
    return {
        "abstract_1": "This is a great scientific abstract that explains the concept of gravity.",
        "abstract_2": "This abstract discusses the fascinating world of quantum mechanics.",
        "abstract_3": "Great progress has been made in the study of artificial intelligence."
    }

def load_concepts():
    # Example concepts dataset
    return {
        "abstract_1": ["great", "gravity"],
        "abstract_2": ["quantum mechanics"],
        "abstract_3": ["artificial intelligence"]
    }

# Dataset class
class AbstractDataset(Dataset):
    def __init__(self, abstracts, concepts, definitions):
        self.data = []

        for abstract_id, abstract in abstracts.items():
            if abstract_id in concepts:
                for concept, definition in definitions.items():
                    if concept in concepts[abstract_id]:
                        label = 1  # Matching definition
                    else:
                        label = 0  # Non-matching definition

                    self.data.append({
                        'abstract': abstract,
                        'concept': concept,
                        'definition': definition,
                        'label': label
                    })

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

# Collate function
def collate_fn(batch, tokenizer, max_length=512):
    all_inputs = []
    all_labels = []

    for item in batch:
        abstract = item['abstract']
        concept = item['concept']
        definition = item['definition']
        label = item['label']

        position = abstract.lower().find(concept.lower())
        if position != -1:
            new_abstract = f"{abstract[:position]} {tokenizer.sep_token} {concept} {tokenizer.sep_token}{abstract[position+len(concept):]}"
        else:
            new_abstract = abstract  # Fallback if concept is not found

        input_text = f"{definition} {tokenizer.sep_token} {new_abstract}"
        all_inputs.append(input_text)
        all_labels.append(label)

    inputs = tokenizer(all_inputs, max_length=max_length, truncation=True, padding=True, return_tensors="pt").to(device)
    labels = torch.tensor(all_labels, dtype=torch.float, device=device)

    return inputs, labels

# Training function
def train():
    # Load pretrained model and tokenizer
    model = AutoModelForSequenceClassification.from_pretrained("microsoft/deberta-base", num_labels=1).to(device)
    tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-base")

    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4)

    # Load data
    abstracts = load_abstracts()
    concepts = load_concepts()
    definitions = {
        "great": "A thing that is totally awesome!",
        "scientific": "Related to science or systematic methods.",
        "gravity": "The force that attracts a body toward the center of the earth.",
        "quantum mechanics": "A fundamental theory in physics describing the properties of nature.",
        "artificial intelligence": "The simulation of human intelligence by machines."
    }

    dataset = AbstractDataset(abstracts, concepts, definitions)
    train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda b: collate_fn(b, tokenizer))

    # Training loop
    model.train()
    for epoch in range(3):
        for batch in train_loader:
            inputs, labels = batch

            outputs = model(**inputs).logits.squeeze(-1)
            loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            print(f"Epoch: {epoch}, Loss: {loss.item()}")

    return model, tokenizer # Added return statement to return the model and tokenizer

model, tokenizer = train()

def predict(model, tokenizer, abstract, concept, definition):
    position = abstract.lower().find(concept.lower())
    if position != -1:
        new_abstract = f"{abstract[:position]} {tokenizer.sep_token} {concept} {tokenizer.sep_token}{abstract[position+len(concept):]}"
    else:
        new_abstract = abstract  # Fallback if concept is not found

    input_text = f"{definition} {tokenizer.sep_token} {new_abstract}"

    inputs = tokenizer(input_text, max_length=512, truncation=True, padding=True, return_tensors="pt").to(device)

    model.eval()
    with torch.no_grad():
        logits = model(**inputs).logits.squeeze(-1)
        probability = torch.sigmoid(logits).item()

    label = 1 if probability >= 0.5 else 0
    return label, probability




# Example usage
example_abstract = "This is a great scientific abstract that explains the concept of gravity."
example_concept = "gravity"
example_definition = "The force that attracts a body toward the center of the earth."

label, probability = predict(model, tokenizer, example_abstract, example_concept, example_definition)
print(f"Predicted Label: {label}, Probability: {probability:.4f}")