# EcoHack-Ontology-Concept-Disambiguation

Technical details:

As the basis for our experiments, we use a subset of 6000 paper titles and abstracts that address invasion biology from a dataset collected using a Wikidata query (https://doi.org/10.5281/zenodo.12518037). We then proceeded to extract concepts from the abstracts by prompting an LLM (Llama-3-8B-Instruct) to identify single-word or multi-word concepts that potentially match concepts contained in ontologies like the ENVO. Then, we again used the same model to generate five definitions for each extracted term, with the corresponding scientific abstract being provided as additional context to ensure that the model generates a definition that explains the actual meaning of the concept as used in the abstract.
As a second part of our dataset, we used the INBIO ontology and the ENVO ontology, which contain the concepts that we want to link to the concepts that occur in the scientific texts. For these concepts from the ontologies, we created five additional definitions using the same LLM. We again ensured that the definitions match the exact meaning of the concept from the ontology by providing the definition for that term that is provided by the ontology as context to the LLM.
The resulting dataset, therefore contains a large number of concepts either from the ontology as well as concepts extracted from abstracts, both in combination with corresponding definitions. We used this dataset to create a pipeline leveraging four different models to perform automatic term matching between ontology and abstracts:

We trained a DeBERTa-base model to recognize relevant concepts (as identified by the LLM) in a scientific abstract. We treated this as a token-level classification problem so that the model could predict scores for each token in the abstract, indicating if it was part of a relevant concept or not. We trained the model to predict label 1 for the first token of every word that the LLM extracted and to predict label 2 for each subsequent token of that concept to determine exact concept boundaries even if two concepts are next to each other. The loss function we used is the categorical cross entropy for the two “positive” labels (1 and 2). For label 0, we decided not to use the categorical cross-entropy since it would strongly punish if additional tokens were selected as relevant if the LLM did not. Since we assume that the LLM might not have extracted all relevant concepts, this would be undesirable. Thus, we instead included an L2 penalty that drives the average probability of labels 1 and 2 to zero for all tokens not marked as relevant by the LLM. This “softer” method of driving relevance scores of non-relevant tokens to 0 ensures that the model is not punished much if it predicts additional concepts as relevant, as long as this is rarely done.
The second model we created is a definition embedding model that transforms definitions of concepts into dense vector representations. We again used the DeBERTa base and trained it by predicting embeddings for two definitions of the same concept (with both originating from the same abstract or the same concept in the ontology to avoid potentially mixing definitions of different concepts that have the same name (e.g., invasion can be used in the medical context, which differs from invasion in ecology)) and training the model to predict similar embeddings for those two definitions, while predicting different embeddings for unrelated concepts. For that, we used a margin-based triplet loss: Given an anchor definition A, a positive sample B (i.e., a definition of the same concept as A,) and a negative definition C (defining a different concept), we defined the loss L as:

We used in-batch negatives and computed this loss for all possible pairs of positive and negative samples in the batch for a single gradient update. The resulting model embeds definitions into an embedding space that places definitions of similar concepts close to each other.
The definition embedding model allows for matching concepts from texts to concepts from the ontology by first generating a definition for the concept from the text, embedding it, and comparing that embedding to the embeddings for the definitions from the concepts in the ontology. This process is not very efficient, though, since it requires generating a definition for each concept of interest in a given text. We, therefore, trained an additional embedding predictor that takes a scientific abstract and directly predicts the embedding of each token, which can then be quickly matched with the ontology embeddings. To do that, we again leveraged our existing dataset: For a given abstract and relevant terms extracted by the LLM, we first embedded the definitions of those concepts using the model trained in the previous section and then used those embeddings as ground truth for training the DeBERTa model (using an L2 loss) to directly predict that embedding for the corresponding token in the abstract without ever seeing the definition. In this way, we created a model that directly predicts semantic embeddings for every relevant concept in the text in a single forward pass.
The resulting models are able to quickly find relevant concepts from the ontology for a given concept in an abstract. A potential problem with this method can be that similar concepts will have similar embeddings so that multiple similar concepts are retrieved without a precise ranking of the relevance since the model never saw both the textual and the ontology concepts at once (because their embeddings are predicted in separate models). For that reason, we trained a reranker model, which is a fine-tuned version of Llama-3.2-1B-Instruct. This model receives three pieces of information as input: 1) a sentence from the abstract, 2) the concept that we are interested in (i.e., a word from the sentence), and 3) a definition. The model is then fine-tuned to return “yes” if the given definition matches the concept from the text and “no” otherwise. The training was done using the definitions generated for the abstract terms as positive samples and random definitions generated for other abstract concepts as negative samples. The resulting model can thus identify if a given concept from the ontology (with corresponding definition) matches a concept from the text.

We created a matching tool that uses all models to perform real-time term matching between user-provided input texts and the INBIO and ENVO ontologies. If a text is provided by the user, the tool first uses the concept extraction model to extract relevant terms, and then uses the abstract token embedding model to generate semantic embeddings for each concept from the abstract. These are then compared to the embeddings of all ontology concepts (which were precomputed using the definition embedding model). Finally, the top 5 candidates that were retrieved in this way for each concept are plugged into the reranker model (together with the sentence in which the concept appears in the text) to create a final ranking of the semantic relatedness of the concepts. This ranking is presented to the user if they click on one of the detected concepts, which we highlight in the text.

Finally, we used our models to create a concept-based scientific search engine. If the user provides an abstract, we again use the abstract token embedding model to generate semantic embeddings for each concept from the abstract so that we essentially have n embeddings of relevant concepts representing a given abstract. We did the same for each abstract in our dataset. To check the relatedness of the two abstracts, we compute the pairwise distances between all concept embeddings that represent the two abstracts. To decide if a candidate paper matches our query paper, we take the minimum distance that each query-paper-embedding has to any other embedding from the candidate so that we get a score for each concept from our query that represents if that concept is also addressed in the other abstract. We then average over all concepts in the query to get a single score that measures the overall concept overlap between the two papers. We found this method to be highly successful at matching related concepts, even if they are not represented by the same words in the abstracts.
